{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Email Spam Filter\n",
    "\n",
    "To classify messages as spam or non-spam, the computer:\n",
    "\n",
    "- Learns how humans classify messages\n",
    "- Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam\n",
    "- Classifies a new message based on these probability values \n",
    "    - If the probability for spam is greater, then it classifies the message as spam\n",
    "    - Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message)\n",
    "\n",
    "So my first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "### The Goal\n",
    "**My goal is to create a spam filter that classifies new messages with an accuracy greater than 80%.**\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the Dataset\n",
    "\n",
    "I'll use some simple Python and pandas operations to explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms_spam = pd.read_csv('SMSSpamCollection', \n",
    "                       sep = '\\t', \n",
    "                       header = None, \n",
    "                       names = ['Label', 'SMS']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "\n",
      "\n",
      "  Label                                                SMS\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "print(sms_spam.shape)\n",
    "print('\\n')\n",
    "print(sms_spam.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Frequencies of Spam and Non-Spam Messages\n",
    "\n",
    "It appears there are two columns of interest: 'Label' and 'SMS'.\n",
    "\n",
    "I'll generate a quick relative frequency table for the values in the 'Label' column to identify how many of the messages in the provided dataset are either spam or ham messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     0.865937\n",
      "spam    0.134063\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(sms_spam['Label'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 87% of the messages are ham, and the remaining 13% are spam. This sample looks representative of emails in general, since in practice most messages that people receive are ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Filter Pre-Creation\n",
    "\n",
    "### Reasoning \n",
    "About 87% of the messages are ham (\"ham\" means non-spam), and the remaining 13% are spam. I can move on to building the spam filter.\n",
    "\n",
    "However, before creating it, it's very helpful to first think of a way of testing how well it works. When creating software (a spam filter is software), a good rule of thumb is that designing the test comes before creating the software. If we write the software first, then it's tempting to come up with a biased test just to make sure the software passes it.\n",
    "\n",
    "Once the spam filter is done, I'll need to test how good it is with classifying new messages. To test the spam filter, I'm going to split our dataset into two categories:\n",
    "\n",
    "- A training set, which we'll use to \"train\" the computer how to classify messages.\n",
    "- A test set, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "I'll keep 80% of our dataset for training, and 20% for testing (to train the algorithm on as much data as possible, but to also have enough test data). The dataset has 5,572 messages, which means that:\n",
    "\n",
    "- The training set will have 4,458 messages (about 80% of the dataset).\n",
    "- The test set will have 1,114 messages (about 20% of the dataset).\n",
    "\n",
    "*To better understand the purpose of putting a test set aside, let's begin by observing that all 1,114 messages in the test set are already classified by a human. When the spam filter is ready, I'll treat these messages as new and have the filter classify them. Once I have the results, I'll be able to compare the algorithm classification with that done by a human, and this way I'll see how good the spam filter really is.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomizing My Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the data set\n",
    "randomized_data = sms_spam.sample(frac = 1, random_state = 1)\n",
    "\n",
    "# create an index for splitting the data set, rounded to avoid decimals\n",
    "splitting_index = round(len(randomized_data) * 0.8) # =4458\n",
    "\n",
    "# using the index to split the data set into training set and testing set\n",
    "training_set = randomized_data[:splitting_index].reset_index(drop = True)\n",
    "testing_set = randomized_data[splitting_index:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the training and testing sets, it's important to check whether or not the sets accurately represent the distribution of ham and spam messages as the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.86541\n",
       "spam    0.13459\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['Label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.868043\n",
       "spam    0.131957\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set['Label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that both the training and testing sets accurately represent the distribution of ham and spam messages as the original set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "After splitting the dataset into a training set and a test set, the next big step is to use the training set to teach the algorithm to classify new messages. To calculate all probabilities, I'll first need to perform a bit of data cleaning. I'll start by removing the punctuation and bringing all the words to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the dataset looks like before cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                                SMS\n",
      "0   ham                       Yep, by the pretty sculpture\n",
      "1   ham      Yes, princess. Are you going to make me moan?\n",
      "2   ham                         Welp apparently he retired\n",
      "3   ham                                            Havent.\n",
      "4   ham  I forgot 2 ask ü all smth.. There's a card on ...\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what our data set looks like after cleaning using some simple string operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                                SMS\n",
      "0   ham                       yep  by the pretty sculpture\n",
      "1   ham      yes  princess  are you going to make me moan \n",
      "2   ham                         welp apparently he retired\n",
      "3   ham                                            havent \n",
      "4   ham  i forgot 2 ask ü all smth   there s a card on ...\n"
     ]
    }
   ],
   "source": [
    "# strip any character not from a-z, A-Z, or 0-9\n",
    "training_set['SMS'] = training_set['SMS'].str.replace('\\W', ' ')\n",
    "\n",
    "# transform every letter in every word To lower case\n",
    "training_set['SMS'] = training_set['SMS'].str.lower()\n",
    "\n",
    "print(training_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Approach to the Problem\n",
    "\n",
    "Everyone gets spam messages and knows how annoying it can be. There are particular defining qualities of spam messages that make them so distinct, and the content of the messages always seems to be similar in nature. \n",
    "\n",
    "I will create a list with all of the unique words that occur in the messages of our training set. The vocabulary should be compiled into a Python list containing all the unique words across all messages, where each word is represented as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming each message into a list split by space characters\n",
    "training_set['SMS'] = training_set['SMS'].str.split()\n",
    "\n",
    "# initiating an empty list\n",
    "vocabulary = []\n",
    "\n",
    "for each_list in training_set['SMS']:\n",
    "    for each_word in each_list:\n",
    "        vocabulary.append(each_word)\n",
    "\n",
    "vocabulary = set(vocabulary) # to get rid of duplicates\n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are 7,783 unique words in all the messages of our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the Dataset\n",
    "\n",
    "After creating the vocabulary for the messages in the training set, I'm going to use the vocabulary to transform the data. I want to create a dictionary that shows how many times each particular word in the vocabulary dictionary is being used. \n",
    "\n",
    "Eventually, I'm going to create a new DataFrame. However, I'll first build a dictionary named 'word_counts_per_sms' that I'll then convert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the New Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each key is a unique word (a string) from the vocabulary\n",
    "# Each value is a list of the length of training set, where each element in the list is a 0\n",
    "word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "# loop over training_set['SMS'] using the enumerate() function to get both the index and the SMS message (index and sms)\n",
    "for index, sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Dictionary into a Pandas DataFrame\n",
    "\n",
    "To visualize what the code above has done, I'm converting the new dictionary into a DataFrame. This new DataFrame shows the count of each respective word found in the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bar</th>\n",
       "      <th>doesnt</th>\n",
       "      <th>accordingly</th>\n",
       "      <th>fifty</th>\n",
       "      <th>8</th>\n",
       "      <th>profit</th>\n",
       "      <th>prakasam</th>\n",
       "      <th>rcvd</th>\n",
       "      <th>isnt</th>\n",
       "      <th>bhaskar</th>\n",
       "      <th>...</th>\n",
       "      <th>ure</th>\n",
       "      <th>broke</th>\n",
       "      <th>pap</th>\n",
       "      <th>tiz</th>\n",
       "      <th>roger</th>\n",
       "      <th>worlds</th>\n",
       "      <th>rayman</th>\n",
       "      <th>oops</th>\n",
       "      <th>buttons</th>\n",
       "      <th>accidentally</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bar  doesnt  accordingly  fifty  8  profit  prakasam  rcvd  isnt  bhaskar  \\\n",
       "0    0       0            0      0  0       0         0     0     0        0   \n",
       "1    0       0            0      0  0       0         0     0     0        0   \n",
       "2    0       0            0      0  0       0         0     0     0        0   \n",
       "3    0       0            0      0  0       0         0     0     0        0   \n",
       "4    0       0            0      0  0       0         0     0     0        0   \n",
       "\n",
       "   ...  ure  broke  pap  tiz  roger  worlds  rayman  oops  buttons  \\\n",
       "0  ...    0      0    0    0      0       0       0     0        0   \n",
       "1  ...    0      0    0    0      0       0       0     0        0   \n",
       "2  ...    0      0    0    0      0       0       0     0        0   \n",
       "3  ...    0      0    0    0      0       0       0     0        0   \n",
       "4  ...    0      0    0    0      0       0       0     0        0   \n",
       "\n",
       "   accidentally  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "\n",
       "[5 rows x 7783 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform word_counts_per_sms into a DataFrame using pd.DataFrame()\n",
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "I'll concatenate the DataFrame I just built above with the DataFrame containing the training set. This way, I'll also have the Label and the SMS columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>bar</th>\n",
       "      <th>doesnt</th>\n",
       "      <th>accordingly</th>\n",
       "      <th>fifty</th>\n",
       "      <th>8</th>\n",
       "      <th>profit</th>\n",
       "      <th>prakasam</th>\n",
       "      <th>rcvd</th>\n",
       "      <th>...</th>\n",
       "      <th>ure</th>\n",
       "      <th>broke</th>\n",
       "      <th>pap</th>\n",
       "      <th>tiz</th>\n",
       "      <th>roger</th>\n",
       "      <th>worlds</th>\n",
       "      <th>rayman</th>\n",
       "      <th>oops</th>\n",
       "      <th>buttons</th>\n",
       "      <th>accidentally</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  bar  doesnt  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]    0       0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...    0       0   \n",
       "2   ham                    [welp, apparently, he, retired]    0       0   \n",
       "3   ham                                           [havent]    0       0   \n",
       "4   ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...    0       0   \n",
       "\n",
       "   accordingly  fifty  8  profit  prakasam  rcvd  ...  ure  broke  pap  tiz  \\\n",
       "0            0      0  0       0         0     0  ...    0      0    0    0   \n",
       "1            0      0  0       0         0     0  ...    0      0    0    0   \n",
       "2            0      0  0       0         0     0  ...    0      0    0    0   \n",
       "3            0      0  0       0         0     0  ...    0      0    0    0   \n",
       "4            0      0  0       0         0     0  ...    0      0    0    0   \n",
       "\n",
       "   roger  worlds  rayman  oops  buttons  accidentally  \n",
       "0      0       0       0     0        0             0  \n",
       "1      0       0       0     0        0             0  \n",
       "2      0       0       0     0        0             0  \n",
       "3      0       0       0     0        0             0  \n",
       "4      0       0       0     0        0             0  \n",
       "\n",
       "[5 rows x 7785 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, I now have a DataFrame with Labels, the content of the messages themselves, and the count of each respective word found in the message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building the Filter with Naive Bayes\n",
    "\n",
    "I'm now done with cleaning the training set and can begin creating the spam filter. The Naive Bayes algorithm will need to answer these two probability questions to be able to classify new messages:\n",
    "\n",
    "$$\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "$$$$\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "$$\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we'll need to use these equations:\n",
    "\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$\n",
    "Some of the terms in the four equations above will have the same value for every new message. I can calculate the value of these terms once and avoid doing the computations again when a new messages comes in. Below, I'll use the training set to calculate:\n",
    "\n",
    "P(Spam) and P(Ham)\n",
    "NSpam, NHam, NVocabulary\n",
    "\n",
    "I'll also use Laplace smoothing and set $\\alpha = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating spam and ham messages first\n",
    "spam_messages = training_set_clean[training_set_clean['Label'] == 'spam']\n",
    "ham_messages = training_set_clean[training_set_clean['Label'] == 'ham']\n",
    "\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = len(spam_messages) / len(training_set_clean)\n",
    "p_ham = len(ham_messages) / len(training_set_clean)\n",
    "\n",
    "# Calculating N_spam, N_ham, N_vocabulary\n",
    "count_of_spam_words_per_sms = spam_messages['SMS'].apply(len)\n",
    "n_spam = count_of_spam_words_per_sms.sum()\n",
    "\n",
    "count_of_ham_words_per_sms = ham_messages['SMS'].apply(len)\n",
    "n_ham = count_of_ham_words_per_sms.sum()\n",
    "\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Parameters\n",
    "\n",
    "Now that I have the constant terms calculated above, I can move on with calculating the parameters $P(w_i|Spam)$ and $P(w_i|Ham)$. Each parameter will thus be a conditional probability value associated with each word in the vocabulary.\n",
    "\n",
    "The parameters are calculated using the formulas:\n",
    "\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate parameters\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_messages[word].sum()\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "    \n",
    "    n_word_given_ham = ham_messages[word].sum()\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see an example of how the dictionaries created now hold all parameters for words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00021764680276846734\n",
      "0.0014303291294986158\n",
      "0.00766116745745005\n",
      "0.0007536142725315288\n"
     ]
    }
   ],
   "source": [
    "print(parameters_spam['hey'])\n",
    "print(parameters_ham['hey'])\n",
    "print(parameters_spam['free'])\n",
    "print(parameters_ham['free'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Created Parameters to Build the Filter\n",
    "\n",
    "Now that I have all our parameters calculated, I can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "- Takes in as input a new message (w1, w2, ..., wn).\n",
    "- Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn).\n",
    "- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "    - If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "    - If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "    - If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Function for Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "    \n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "            \n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "            \n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "    \n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Filter Demonstrated\n",
    "\n",
    "Below, we demonstrate the created spam filter in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.3481290211300841e-25\n",
      "P(Ham|message): 1.9368049028589875e-27\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 2.4372375665888117e-25\n",
      "P(Ham|message): 3.687530435009238e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "classify(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Filter on the Test Set\n",
    "\n",
    "The two results above look promising, but let's see how well the filter does on the test set, which has 1,114 messages.\n",
    "\n",
    "I'll start by writing a function that returns classification labels instead of printing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a function that returns labels instead of printing them, I can use it to create a new column in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS predicted\n",
       "0   ham          Later i guess. I needa do mcat study too.       ham\n",
       "1   ham             But i haf enuff space got like 4 mb...       ham\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...      spam\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...       ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...       ham"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set['predicted'] = testing_set['SMS'].apply(classify_test_set)\n",
    "testing_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Filter on the Test Set\n",
    "\n",
    "The filter is created and applied to the test set. This data has been labeled but was not used in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Categorization: 1100\n",
      "Incorrect Categorization: 14\n",
      "Filter Accuracy: 98.74 %\n"
     ]
    }
   ],
   "source": [
    "# measure the accuracy of the spam filter\n",
    "\n",
    "correct = 0\n",
    "total = len(testing_set)\n",
    "\n",
    "for each_row in testing_set.iterrows():\n",
    "    row_data = each_row[1]\n",
    "    if row_data['Label'] == row_data['predicted']:\n",
    "        correct += 1\n",
    "\n",
    "print('Correct Categorization:', correct)\n",
    "print('Incorrect Categorization:', total - correct)\n",
    "print('Filter Accuracy:', round(correct/total * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is close to 98.74%, which is really good. The spam filter looked at 1,114 messages that it hasn't seen in training, and classified 1,100 correctly.\n",
    "\n",
    "### Next Steps\n",
    "In this project, I managed to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. The filter had an accuracy of 98.74% on the test set I used, which is a pretty good result. The initial goal was an accuracy of over 90%, and I managed to do much better than that.\n",
    "\n",
    "Future steps include:\n",
    "\n",
    "- Analyze the 14 messages that were classified incorrectly and try to figure out why the algorithm classified them incorrectly\n",
    "- Make the filtering process more complex by making the algorithm sensitive to letter case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
